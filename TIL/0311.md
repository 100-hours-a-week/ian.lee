단어	정리
NLP	vicky: 컴퓨터가 인간의 언어를 이해하고 생성할 수 있도록 분석, 변환을 진행하는 방법.

dain: 인간의 언어를 컴퓨터가 이해할 수 있도록 만드는 것

kai: 인간의 언어(자연어)를 활용하여 인공지능 모델을 통해 감성 분석, 번역, 텍스트 제네레이션 등의 작업을 수행하는 것이다.

ian:  자연어를 표준화와 정규화 과정을 거쳐 인간의 언어 현상을 컴퓨터와 같은 기계를 이용해서 묘사할 수 있도록 연구하고 이를 구현하는 인공지능

tei : 자연어를 컴퓨터에게 이해시키고 처리할 수 있도록 하는 과정

최종 : 자연어를 표준화와 정규화 과정을 거쳐 컴퓨터가 인간의 언어를 이해할 수 있도록  한 인공지능
RNN	vicky: 순차적(시계열) 데이터를 학습할때, 이전 상태를 고려하여 다음 출력을 생성하는 인공신경망 모델.

dain: 데이터가 순차적으로 입력되면서 이전 데이터에서 계산한 값을 바탕으로 가중치를 업데이트하는 것. 

kai: 순차적인 입력 데이터에 대한 패턴을 학습하기에 적합한 인공 신경망으로 이전 입력 데이터에 대한 파라미터를 재사용 하여 가중치를 업데이트 하는 방식으로 구동된다.

ian:  순차적인 데이터가 들어오면 시간의 흐름에 따라 데이터를 입력하면 예측을 하고 이전 시점의 정보를 현재 시점으로 전달하는 중간 상태로, 시간에 따른 문맥 정보를 모델 내에서 유지하고 다음을 예측하는 것

tei : 시간의 흐름이나 언어처럼 순서가 있는 데이터를 활용하여 이전 데이터로 다음 데이터를 예측 및 학습하는 모델

최종 : 순차적인 시간의 흐름에 따라  데이터를 입력하려면 이전 상태(데이터)를 활용하여 파라미터를 재사용하여 가중치를 업데이트하고 다음 데이터를 예측하는 모델이다. 
LSTM	vicky: RNN에 장/단기 기억을 추가한 모델로, 기울기 소실문제를 해결하기 위해 장기 의존성을 학습하는 인공신경망 모델.

dain: 이전에 입력된 데이터가 현재 입력되는 데이터와 유사하다면, 이전에 입력된 데이터도 가중치 업데이트에 활용하는 것. 

kai: 일반 RNN에 gate 메커니즘을 추가하여, vanishing gradient 문제를 개선하고 더 긴 정보의 기억을 유지할 수 있도록 설계된 시계열 모델이다.

ian:  RNN의 장기 의존성(기울기 소실 문제)을 해결하기 위해 고안된 모델로, 메모리 셀과 망각, 입력, 출력 게이트를 활용하여 긴 시간 동안 필요한 정보를 선택적으로 유지하거나 잊고, 새로운 정보를 반영함으로써 효과적으로 장기적인 정보를 학습하고 예측할 수 있도록 만든 구조

tei : 중요 데이터와 중요하지 않은 데이터를 구분하여 기억 및 망각을 통해 장기 기억을 하는 모델로 RNN과 마찬가지로 순서가 있는 데이터를 학습하는 모델

최종 : RNN모델과 마찬가지로 순서가 있는 데이터를 학습하는 모델이며, RNN의 기울기 소실 문제를 해결하기 위해 게이트 구조로 장기 의존성을 학습하는 인공 신경망. 
Word embedding	vicky: 단어를 수치화 및 벡터화하여 의미적 유사성을 표현한 것.

dain: 단어를 수치 연산을 할 수 있는 벡터로 바꾸는 것

kai: 단어 뭉치(corpus) 내의 토큰 별로 일정한 차원의 형태로 벡터화를 수행하여 NLP 모델의 입력 데이터롤 활용할 수 있게 수치화 하는 과정이다.

ian:  단어를 의미적으로 유사한 단어끼리 가까운 거리의 벡터 공간상에 표현하여 컴퓨터가 자연어를 효율적으로 처리할 수 있도록 만들어 주는 기법

tei : 미리 학습된 벡터 공간을 활용해 단어를 벡터화하는 과정으로 유사한 단어는 가까운 벡터 공간에 위치하게 되어 학습에 용이하다.

최종 : 문장 내 토큰을 수치화 하기 위해 벡터로 변환하여 컴퓨터가 NLP 작업을 수행할 수 있도록 전처리 하는 과정
GAN	vicky: 데이터를 생성하는 모델과 데이터를 판별하는 모델을 서로 경쟁시켜 학습하는 인공 신경망 모델.

dain: 한 신경망은 가짜 이미지를 생성하고, 다른 신경망은 가짜 이미지를 판별하면서 서로 경쟁하며 모델 정확도를 높이는 것.

kai: 원본 데이터로 부터 노이즈가 더해준 실제와 유사한 데이터를 생성하여 원본과 생성본 간의 차이를 판별하여 가짜 데이터가 잘 생성될 수 있는 방향으로 설계된 모델이다.

ian:  생성자(Generator)와 판별자(Discriminator)가 경쟁적으로 학습하면서 실제 데이터와 유사한 새로운 데이터를 생성하도록 하는 비지도학습 기반의 신경망 모델

tei : Generator와 Discriminator 두 모델의 경쟁을 통한 성능 향상을 노릴 수 있는 모델

최종 : 생성자는 노이즈로부터 실제와 유사한 데이터를 생성하고 판별자는 이를 판별하는 과정을 통해 성능을 향상시키는 비지도학습 기반 신경망 모델
Transformer 
model	vicky: 셀프 어텐션과 feed-forward를 사용하여 순차적 단어 학습이 아닌, 병렬적 단어학습을 통해 자연어 처리의 효율을 높인 모델.

dain: 어텐션 메커니즘을 활용하여 자연어를 처리하는 모델

kai: 기존 시게열 모델이 갖고 있는 Context Vector의 정보 손실 문제를 Attention Mechanism만을 활용하여 해결한 모델로 NLP분야에서 등장하는 대부분의 SOTA 모델이 근간이 된다.

ian:  어텐션 메커니즘을 기반으로 시퀀스 내에서 단어 간 관계를 효과적으로 파악하여 병렬적인 처리하는데 인코더에서 어느 부분이 서로 관련 있는지 정의하고 다음 인코더로 보낸다 디코더는 모든 인코딩을 취하고 파생된 context를 사용하여 출력 시퀀스를 생성한다

tei : RNN, LSTM과 다르게 Positional Encoding을 통해 병렬 처리를 하며, self-attention을 통한 단어의 관계를 학습하며 Masked Self-Attention을 통해 앞선 데이터만을 이용하여 학습 및 예측하는 모델

최종 : Self-Attention과 Feed-Forward Network를 활용하여 병렬 학습을 수행하며, Positional Encoding을 통해 문맥 정보를 보완한 모델로 인코더는 입력 문장에서 관계를 학습하고 디코더는 인코딩된 정보를 바탕으로 Masked Self-Attention을 적용하여 순차적으로 출력하여 기존 시계열 모델의 정보 손실 문제를 해결하는 모델.
Attention
mechanism	vicky: 입력데이터의 각 시퀀스와 출력데이터간의 관계를 계산하여 중요한 요소에 집중할 수 있게 하는 방법.

dain: 시퀀스의 모든 요소가 자기 자신과 다른 요소와의 관계를 고려하여 자기 자신을 다시 계산하는 메커니즘. 

kai: 입력 문장 내의 토큰 간의 관계를 나타내기 위해 모든 토큰 간의 내적연산을 통해 얼마나 연관성이 있는지 도출하는 방법이다.

ian:  입력 데이터 중에서 모델이 예측이나 판단에 중요한 부분에 집중할 수 있도록 각 부분의 중요도를 가중치 형태로 계산하고 반영하는 것

tei : RNN, LSTM에서의 초반 정보가 레이어를 지날 수록 사라지는 문제를 해결하며 중요한 단어에 집중할 수 있도록한 개념

최종 : 입력 문장 내의 단어 간의 관계(중요도)를 계산하기 위해 모든 단어 끼리 내적 연산을 수행하는 것

